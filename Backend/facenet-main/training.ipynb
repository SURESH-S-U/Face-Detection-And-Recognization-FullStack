{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FACELOADING:\n",
    "    \"\"\"\n",
    "    A class for loading and processing face images using YOLOv8 face detection.\n",
    "    This class handles loading images from a directory structure, detecting faces,\n",
    "    and preparing them for further processing like face recognition.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, directory):\n",
    "        \"\"\"\n",
    "        Initialize the FACELOADING class.\n",
    "        \n",
    "        Args:\n",
    "            directory (str): Root directory containing subdirectories of face images\n",
    "        \"\"\"\n",
    "        self.directory = directory\n",
    "        self.target_size = (160, 160)  # Standard size for face recognition\n",
    "        self.X = []  # Will store face images\n",
    "        self.Y = []  # Will store corresponding labels\n",
    "        # Initialize YOLOv8 face detector with the face detection model\n",
    "        self.detector = YOLO('yolov8l-face.pt')\n",
    "\n",
    "    def extract_face(self, filename):\n",
    "        \"\"\"\n",
    "        Extract a face from a single image using YOLOv8.\n",
    "        \n",
    "        Args:\n",
    "            filename (str): Path to the image file\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Processed face image resized to target_size\n",
    "        \"\"\"\n",
    "        # Read and convert image to RGB (YOLOv8 works with RGB)\n",
    "        img = cv.imread(filename)\n",
    "        img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Perform face detection\n",
    "        results = self.detector.predict(img, conf=0.5)[0]\n",
    "        \n",
    "        # Get the first detected face (assumes one face per image)\n",
    "        if len(results.boxes) > 0:\n",
    "            # Extract bounding box coordinates\n",
    "            box = results.boxes[0]\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            \n",
    "            # Extract and resize face region\n",
    "            face = img[y1:y2, x1:x2]\n",
    "            face_arr = cv.resize(face, self.target_size)\n",
    "            return face_arr\n",
    "        else:\n",
    "            raise Exception(\"No face detected in the image\")\n",
    "\n",
    "    def load_faces(self, dir):\n",
    "        \"\"\"\n",
    "        Load all faces from a directory.\n",
    "        \n",
    "        Args:\n",
    "            dir (str): Directory containing face images\n",
    "            \n",
    "        Returns:\n",
    "            list: List of processed face images\n",
    "        \"\"\"\n",
    "        faces = []\n",
    "        for im_name in os.listdir(dir):\n",
    "            try:\n",
    "                # Construct full path and process image\n",
    "                path = os.path.join(dir, im_name)\n",
    "                single_face = self.extract_face(path)\n",
    "                faces.append(single_face)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {im_name}: {str(e)}\")\n",
    "        return faces\n",
    "\n",
    "    def load_classes(self):\n",
    "        \"\"\"\n",
    "        Load all classes (subjects) from the main directory.\n",
    "        Each subdirectory name is treated as a class label.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (numpy.ndarray of face images, numpy.ndarray of labels)\n",
    "        \"\"\"\n",
    "        for sub_dir in os.listdir(self.directory):\n",
    "            # Construct path for each subject's directory\n",
    "            path = os.path.join(self.directory, sub_dir)\n",
    "            if os.path.isdir(path):\n",
    "                # Load all faces for current subject\n",
    "                faces = self.load_faces(path)\n",
    "                # Create labels for all faces of current subject\n",
    "                labels = [sub_dir for _ in range(len(faces))]\n",
    "                print(f\"Loaded {len(labels)} images for subject: {sub_dir}\")\n",
    "                # Extend our collections\n",
    "                self.X.extend(faces)\n",
    "                self.Y.extend(labels)\n",
    "        \n",
    "        return np.asarray(self.X), np.asarray(self.Y)\n",
    "\n",
    "    def plot_images(self):\n",
    "        \"\"\"\n",
    "        Plot all processed face images in a grid layout.\n",
    "        \"\"\"\n",
    "        if not self.X:\n",
    "            print(\"No images loaded to plot\")\n",
    "            return\n",
    "            \n",
    "        plt.figure(figsize=(18, 16))\n",
    "        ncols = 3\n",
    "        nrows = len(self.Y) // ncols + 1\n",
    "        \n",
    "        for num, image in enumerate(self.X):\n",
    "            plt.subplot(nrows, ncols, num + 1)\n",
    "            plt.imshow(image)\n",
    "            plt.title(self.Y[num], pad=10)\n",
    "            plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_loading=FACELOADING(r'faces_data')\n",
    "X,y=face_loading.load_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_loading.plot_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary FaceNet model from keras_facenet\n",
    "from keras_facenet import FaceNet\n",
    "\n",
    "# Initialize the FaceNet model. It will automatically load the pre-trained weights.\n",
    "embedder = FaceNet()\n",
    "\n",
    "def get_embedding(face_img):\n",
    "    # Convert the input face image to a 32-bit floating point format\n",
    "    # TensorFlow models typically expect images to be in a float format.\n",
    "    face_img = face_img.astype('float32')  # 3D image (160x160x3)\n",
    "\n",
    "    # Add an extra dimension to the image to create a batch of size 1\n",
    "    # This is because the model expects input as a batch, even if it's just one image.\n",
    "    # So we expand the dimensions of the image to (1, 160, 160, 3).\n",
    "    # Before: (160, 160, 3), After: (1, 160, 160, 3)\n",
    "    face_img = np.expand_dims(face_img, axis=0)  # 4D (None, 160, 160, 3)\n",
    "    \n",
    "    # Pass the prepared image through the FaceNet model to get the embedding (feature vector)\n",
    "    # The model will output a 512-dimensional vector (1x512) for the given face image\n",
    "    yhat = embedder.embeddings(face_img)\n",
    "\n",
    "    # Return the embedding of the first (and only) image in the batch\n",
    "    # The output is a 512-dimensional vector, which represents the features of the face\n",
    "    return yhat[0]  # 512D vector for the image (1x1x512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDED_X = []\n",
    "\n",
    "# Iterate over each image in X\n",
    "for img in X:\n",
    "    # For each image, get the embedding (feature vector) using the get_embedding function\n",
    "    EMBEDDED_X.append(get_embedding(img))\n",
    "\n",
    "# Convert the list of embeddings into a NumPy array\n",
    "EMBEDDED_X = np.asarray(EMBEDDED_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('faces_embeddings.npz', EMBEDDED_X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the LabelEncoder object\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder to the labels (y) and then transform them into numerical format\n",
    "encoder.fit(y)\n",
    "\n",
    "# Transform the original labels (y) into numerical values\n",
    "y = encoder.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(EMBEDDED_X[0]) \n",
    "plt.ylabel(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(EMBEDDED_X, y, shuffle=True, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initialize the Support Vector Machine model with a linear kernel\n",
    "model = SVC(kernel='linear', probability=True)\n",
    "\n",
    "# Train (fit) the model using the training data\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypreds_train = model.predict(X_train)\n",
    "ypreds_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(Y_train, ypreds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(Y_test,ypreds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#save the model\n",
    "with open('svm_model_160x160.pkl','wb') as f:\n",
    "    pickle.dump(model,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face_recognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
